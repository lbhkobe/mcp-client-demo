server:
  port: 8080

spring:
  application:
    name: mcp-client
  ai:
    ollama: # 引入ollama配置
      base-url: http://localhost:11434
      init:
        pull-model-strategy: never # 是否在启动时拉取模型以及如何拉取。
        timeout: 10s # 拉取模型超时时间。
      chat:
        model: qwen2.5:0.5b #deepseek-r1:latest #要使用的支持的模型的名称。
    model: # ai模型选择
      chat: ollama # 启用Ollama聊天模型。
    mcp: # 启用MCP
      client:  # MCP CLIENT配置
        type: async
        request-timeout: 10s
#        stdio:
#         servers-configuration: classpath:/mcp-servers-config.json
        sse:
          connections:
            server1:
                url: https://mcp.amap.com
                sse-endpoint: /sse?key=2549fa069752fb0ec89ed9e409690044
            msc-product-mcp-server:
                url: http://localhost:8082
#                sse-endpoint: /mcp/queryProduct
#            server3:
#               url: https://mcp-09724909-442f-4b85.api-inference.modelscope.cn


logging:
  level:
    com.example.mcpserver: INFO
    org.springframework.ai: INFO